{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import json\n",
    "import tempfile\n",
    "import platform\n",
    "import subprocess\n",
    "import statistics\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from anacore.msi import MSISample, LocusRes, LocusResPairsCombi, MSILocus, MSIReport, LocusClassifier\n",
    "from anacore.msings import MSINGSAnalysis, MSINGSReport\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeSeries(series):\n",
    "    count_by_val = dict()\n",
    "    for curr_series in series:\n",
    "        for val in curr_series:\n",
    "            if val in count_by_val:\n",
    "                count_by_val[val] += 1\n",
    "            else:\n",
    "                count_by_val[val] = 1\n",
    "    new_series = list()\n",
    "    for val, count in count_by_val.items():\n",
    "        for idx in range(count):\n",
    "            new_series.append(val)\n",
    "    new_series = np.asarray(new_series)\n",
    "    return new_series\n",
    "\n",
    "def getStacked(array):\n",
    "    nb_by_val = {}\n",
    "    for curr_val in array:\n",
    "        curr_val = str(curr_val)\n",
    "        if curr_val in nb_by_val:\n",
    "            nb_by_val[curr_val] += 1\n",
    "        else:\n",
    "             nb_by_val[curr_val] = 1\n",
    "    return nb_by_val\n",
    "\n",
    "def getUnstacked(nb_by_val):\n",
    "    array = []\n",
    "    for val, nb in nb_by_val.items():\n",
    "        for idx in range(nb):\n",
    "            array.append(int(val))\n",
    "    return array\n",
    "\n",
    "def getMSISamples(in_folder):\n",
    "    samples_res = list()\n",
    "    for filename in os.listdir(in_folder):\n",
    "            filepath = os.path.join(in_folder, filename)\n",
    "            data = None\n",
    "            with open(filepath) as FH:\n",
    "                data = json.load(FH)[0]\n",
    "            curr_spl = MSISample.fromDict(data)\n",
    "            samples_res.append(curr_spl)\n",
    "    return samples_res\n",
    "\n",
    "def getExpectedStatus(in_status, in_separator=\"\\t\"):\n",
    "    status_by_spl = dict()\n",
    "    with open(in_status) as FH_status:\n",
    "        for line in FH_status:\n",
    "            if not line.startswith(\"#\"):\n",
    "                spl, date, report_id, commentary, spl_status, BAT25_status, BAT26_status, NR21_status, NR22_status, NR24_status, NR27_status, HT17_status, commentary_2 = [elt.strip() for elt in line.split(in_separator)]\n",
    "                if \"normal\" not in commentary and \"sain\" not in commentary:\n",
    "                    status_by_spl[spl] = {\n",
    "                        \"MSI_BAT25\": BAT25_status,\n",
    "                        \"MSI_BAT26\": BAT26_status,\n",
    "                        \"MSI_NR21\": NR21_status,\n",
    "                        \"MSI_NR22\": NR22_status,\n",
    "                        \"MSI_NR24\": NR24_status,\n",
    "                        \"MSI_NR27\": NR27_status,\n",
    "                        \"MSI_HT17\": HT17_status,\n",
    "                        \"sample\": spl_status\n",
    "                    }\n",
    "    return status_by_spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_score_custom(locus, whole_data, whole_labels, classifiers, cv):\n",
    "    rows = []\n",
    "    titles = [\"locus\", \"dataset_id\", \"train_nb_MSS\", \"train_nb_MSI\", \"test_nb_MSS\", \"test_nb_MSI\", \"%_error_MSS\", \"%_error_MSI\", \"method\", \"accuracy\"]\n",
    "    trainnig_id = 0\n",
    "    for train_idx, test_idx in cv.split(whole_data, groups=whole_labels):\n",
    "        trainnig_id += 1\n",
    "        # Split dataset in train and test sub-datasets\n",
    "        train = {\"names\": [], \"data\": [], \"labels\": []}\n",
    "        test = {\"names\": [], \"data\": [], \"labels\": []}\n",
    "        for data_idx, (data, label) in enumerate(zip(whole_data, whole_labels)):\n",
    "            lib_name = data.name.replace(\"_L001\", \"\")\n",
    "            spl_name = lib_name.split(\"_\")[0]\n",
    "            if data_idx in train_idx:\n",
    "                train[\"names\"].append(lib_name)\n",
    "                train[\"data\"].append(data)\n",
    "                train[\"labels\"].append(label)\n",
    "            elif data_idx in test_idx:\n",
    "                test[\"names\"].append(lib_name)\n",
    "                test[\"data\"].append(data)\n",
    "                test[\"labels\"].append(label)\n",
    "        # Launch classifier\n",
    "        for clf in classifiers:\n",
    "            predictions = []\n",
    "            if clf.method_name == \"MSINGS\":\n",
    "                if platform.system() == \"Linux\":\n",
    "                    clf.fit(train[\"data\"])\n",
    "                    predictions = clf.predict(test[\"data\"])\n",
    "                    clf.clearTmp()\n",
    "            else:\n",
    "                clf.fit(train[\"data\"])\n",
    "                predictions = clf.predict(test[\"data\"])\n",
    "            if len(predictions) != 0:\n",
    "                accuracy = getAccuracy(*getDetermined(predictions, test[\"labels\"]))\n",
    "                err_by_status = getNbErrByStatus(predictions, test[\"labels\"])\n",
    "                err_MSS = 0 if \"MSS\" not in err_by_status else err_by_status[\"MSS\"]\n",
    "                err_MSI = 0 if \"MSI\" not in err_by_status else err_by_status[\"MSI\"]\n",
    "                rows.append([\n",
    "                    locus[\"name\"],\n",
    "                    trainnig_id,\n",
    "                    train[\"labels\"].count(\"MSS\"),\n",
    "                    train[\"labels\"].count(\"MSI\"),\n",
    "                    test[\"labels\"].count(\"MSS\"),\n",
    "                    test[\"labels\"].count(\"MSI\"),\n",
    "                    (100 * err_MSS)/test[\"labels\"].count(\"MSS\"),\n",
    "                    (100 * err_MSI)/test[\"labels\"].count(\"MSI\"),\n",
    "                    clf.method_name,\n",
    "                    accuracy\n",
    "                ])\n",
    "                \n",
    "    return pd.DataFrame.from_records(rows, columns=titles)     \n",
    "\n",
    "\n",
    "def getDetermined(predictions, labels):\n",
    "    cleaned_pred = list()\n",
    "    cleaned_lab = list()\n",
    "    for observed, expected in zip(predictions, labels):\n",
    "        if expected != \"Undetermined\":\n",
    "            cleaned_pred.append(observed)\n",
    "            cleaned_lab.append(expected)\n",
    "    return cleaned_pred, cleaned_lab\n",
    "\n",
    "\n",
    "def getNbErrByStatus(predictions, labels):\n",
    "    nb_err = {elt: 0 for elt in set(labels)}\n",
    "    for expected, observed in zip(labels, predictions):\n",
    "        for status in nb_err:\n",
    "            if expected == status:\n",
    "                if expected != observed:\n",
    "                    nb_err[status] += 1\n",
    "    return nb_err\n",
    "\n",
    "\n",
    "def getAccuracy(predictions, labels):\n",
    "    nb_true = 0\n",
    "    nb_false = 0\n",
    "    for idx, predicted in enumerate(predictions):\n",
    "        expected = labels[idx]\n",
    "        if expected == predicted:\n",
    "            nb_true += 1\n",
    "        else:\n",
    "            nb_false += 1\n",
    "    # Calculate accuracy   \n",
    "    return nb_true/(nb_true + nb_false)\n",
    "\n",
    "\n",
    "class MSINGSClassifier:\n",
    "    def __init__(self, msings_env, msi_path, locus_id):\n",
    "        self.locus_id = locus_id\n",
    "        self.method_name = \"MSINGS\"\n",
    "        self.model_method_name = \"model\"\n",
    "        self.msings_env = msings_env\n",
    "        self.msi_path = msi_path\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.working_directory = os.path.join(tempfile.gettempdir(), self.id)\n",
    "        self.train_directory = os.path.join(self.working_directory, \"train\")\n",
    "        self.predict_directory = os.path.join(self.working_directory, \"predict\")\n",
    "        self.baseline_path = os.path.join(self.train_directory, \"model.tsv\")\n",
    "        self.classes_ = [\"MSI\", \"MSS\"]\n",
    "        \n",
    "    def fit(self, data):\n",
    "        if not os.path.exists(self.train_directory):\n",
    "            os.makedirs(self.train_directory)\n",
    "        # Analyse training samples\n",
    "        tmp_files = []\n",
    "        for msi_idx, msi in enumerate(data):\n",
    "            if msi.loci[self.locus_id].results[self.model_method_name].status == \"MSS\":  # Only the MSS are required for baseline creation\n",
    "                model_path = os.path.join(self.train_directory, \"spl{}.msi.txt\".format(msi_idx))\n",
    "                tmp_files.append(model_path)\n",
    "                with MSINGSAnalysis(model_path, \"w\") as FH_analyze:\n",
    "                    FH_analyze.write(msi.loci[self.locus_id])\n",
    "        # Create model\n",
    "        cmd = [msings_env, msi_path, \"create_baseline\", self.train_directory, \"-o\", self.baseline_path]\n",
    "        subprocess.check_call(cmd)\n",
    "        # Removes tmp analyses\n",
    "        for curr_tmp in tmp_files:\n",
    "            if os.path.exists(curr_tmp):\n",
    "                os.remove(curr_tmp)\n",
    "        return self\n",
    "            \n",
    "    def predict(self, data):\n",
    "        classes = []\n",
    "        idx_by_class = {cls:idx for idx, cls in enumerate(self.classes_)}\n",
    "        if not os.path.exists(self.predict_directory):\n",
    "            os.makedirs(self.predict_directory)\n",
    "        for msi_idx, msi in enumerate(data):\n",
    "            msi_locus = msi.loci[self.locus_id]\n",
    "            # Classify\n",
    "            analyze_path = os.path.join(self.predict_directory, \"spl{}.msi.txt\".format(msi_idx))\n",
    "            report_path = os.path.join(self.predict_directory, \"spl{}_report.tsv\".format(msi_idx))\n",
    "            with MSINGSAnalysis(analyze_path, \"w\") as FH_analyze:\n",
    "                FH_analyze.write(msi_locus)\n",
    "            cmd = [self.msings_env, self.msi_path, \"count_msi_samples\", self.baseline_path, self.predict_directory, \"-m\", \"2.0\", \"-t\", \"0.2\", \"0.2\", \"-o\", report_path]\n",
    "            subprocess.check_call(cmd)\n",
    "            # mSINGS parse results\n",
    "            report = MSINGSReport(report_path)\n",
    "            predicted_status = report.samples[\"spl{}\".format(msi_idx)].loci[msi_locus.position].results[\"MSINGS\"].status\n",
    "            classes.append(predicted_status)\n",
    "            # Removes tmp files\n",
    "            for curr_tmp in [analyze_path, report_path]:\n",
    "                if os.path.exists(curr_tmp):\n",
    "                    os.remove(curr_tmp)\n",
    "        return classes\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.clearTmp()\n",
    "    \n",
    "    def clearTmp(self):\n",
    "        if os.path.exists(self.working_directory):\n",
    "            shutil.rmtree(self.working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "min_nb_reads = 300\n",
    "min_nb_reads_models = 800\n",
    "init_random_seed = 8852\n",
    "method_name = \"MIAmSClassif\"\n",
    "\n",
    "msings_env = os.path.join(os.getcwd(), \"msings\", \"msings-env\", \"bin\", \"python\")\n",
    "msi_path = os.path.join(os.getcwd(), \"msings\", \"msi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locus models from runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Models for MSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expected status\n",
    "status_path = os.path.join(os.getcwd(), \"data\", \"status_by_spl.tsv\")\n",
    "status_by_spl = getExpectedStatus(status_path)\n",
    "\n",
    "# Import results\n",
    "data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "msi_eval = []\n",
    "runs_parent_folder = os.path.join(data_folder, \"MIAmS_1.0.0-b4\")\n",
    "for run_folder in os.listdir(runs_parent_folder):\n",
    "    filepath = os.path.join(runs_parent_folder, run_folder)\n",
    "    msi_eval.extend(getMSISamples(os.path.join(filepath, \"data\")))\n",
    "\n",
    "# Get loci\n",
    "loci = set()\n",
    "loci_id_by_name = {}\n",
    "for spl in msi_eval:\n",
    "    for locus_id in spl.loci:\n",
    "        loci.add(locus_id)\n",
    "        locus_name = spl.loci[locus_id].name\n",
    "        loci_id_by_name[locus_name] = locus_id\n",
    "\n",
    "# Get models params\n",
    "models_param = list()\n",
    "missing_spl = set()\n",
    "for locus_id in sorted(loci):\n",
    "    means = list()\n",
    "    std_dev = list()\n",
    "    locus_name = msi_eval[0].loci[locus_id].name\n",
    "    for spl in msi_eval:\n",
    "        diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "        if diamic_name in status_by_spl:\n",
    "            if status_by_spl[diamic_name][locus_name] == \"MSS\" and spl.loci[locus_id].results[method_name].getNbFrag() * 2 >= min_nb_reads_models:\n",
    "                series = getUnstacked(spl.loci[locus_id].results[method_name].data[\"nb_by_length\"])\n",
    "                means.append(statistics.mean(series))\n",
    "                std_dev.append(statistics.stdev(series))\n",
    "                sns.kdeplot(series, label=spl.name, legend=False, bw=0.5)\n",
    "        else:\n",
    "            missing_spl.add(diamic_name)\n",
    "    if len(means) > 3:\n",
    "        print(\"{}: {:0.2f} (+/- {:0.2f}) on {} samples\".format(locus_name, statistics.median(means), statistics.median(std_dev), len(means)))\n",
    "        models_param.append({\n",
    "            \"name\": locus_name,\n",
    "            \"id\": locus_id,\n",
    "            \"mean\": round(statistics.median(means), 0),\n",
    "            \"std_dev\": round(statistics.median(std_dev), 2),\n",
    "            \"simulated_data\": []\n",
    "        })\n",
    "    else:\n",
    "        print(\"{}: ? (+/- ?)\".format(locus_name))\n",
    "\n",
    "    plt.title(\"{} {} samples\".format(locus_name, len(means)))\n",
    "    plt.show()\n",
    "print(\"\\nSamples not in status database {}\".format(sorted(missing_spl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les profils étudiés dans les runs sont très similaires si l'on se concentre sur ceux contenant plus de {{min_nb_reads}} reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation at locus level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Classification accuracy and stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classify data with ShuffleSplit\n",
    "accuracy_locus_df = []\n",
    "for locus in models_param:\n",
    "    # Classifiers\n",
    "    used_classifiers = [\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Logistic Regression\", LogisticRegression(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Decision Tree\", DecisionTreeClassifier(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"K-Neighbors\", KNeighborsClassifier(2), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Gaussian Naive Bayes\", GaussianNB(), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"C-Support Vector\", SVC(random_state=init_random_seed, probability=True), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest\", RandomForestClassifier(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest 20\", RandomForestClassifier(random_state=init_random_seed, n_estimators=20), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest 30\", RandomForestClassifier(random_state=init_random_seed, n_estimators=30), data_method_name=method_name\n",
    "        ),\n",
    "        MSINGSClassifier(msings_env, msi_path, locus[\"id\"])\n",
    "    ]\n",
    "    \n",
    "    # Select data\n",
    "    whole_labels = []\n",
    "    whole_data_obj = []\n",
    "    for spl in msi_eval:\n",
    "        if spl.loci[locus[\"id\"]].results[method_name].getNbFrag() * 2 >= min_nb_reads:\n",
    "            diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "            if diamic_name in status_by_spl:\n",
    "                status = status_by_spl[diamic_name][locus[\"name\"]]\n",
    "                if status != \"Undetermined\":\n",
    "                    whole_labels.append(status)\n",
    "                    whole_data_obj.append(spl)\n",
    "                    spl.loci[locus[\"id\"]].results[\"model\"] = LocusResPairsCombi(status, data=spl.loci[locus[\"id\"]].results[method_name].data)\n",
    "    print(\"{}: {} stables, {} instables\".format(locus[\"name\"], whole_labels.count(\"MSS\"), whole_labels.count(\"MSI\")))\n",
    "    \n",
    "    # Calculates results\n",
    "    cv = ShuffleSplit(n_splits=200, test_size=0.4, random_state=init_random_seed)\n",
    "    accuracy_locus_df.append(\n",
    "        cross_val_score_custom(locus, whole_data_obj, whole_labels, used_classifiers, cv)\n",
    "    )\n",
    "accuracy_df = pd.concat(accuracy_locus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset information\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"train_nb_MSS\", hue=\"method\", data=accuracy_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Number of MSS in train dataset\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"train_nb_MSI\", hue=\"method\", data=accuracy_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Number of MSI in train dataset\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"test_nb_MSS\", hue=\"method\", data=accuracy_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Number of MSS in test dataset\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"test_nb_MSI\", hue=\"method\", data=accuracy_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Number of MSI in test dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le nombre d'échantillons stables et instables est très limite dans une partie importante des jeux de test de BAT25. Il faudra en tenir compte dans les conclusions de cette étude en limitant l'importance des résultats obtenus sur ce marqueur.\n",
    "* Globalement nous avons peu de données pour de l'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "accuracy_plt = sns.factorplot(x=\"locus\", y=\"accuracy\", hue=\"method\", data=accuracy_df, kind=\"bar\", size=7)\n",
    "for ax in accuracy_plt.axes.flat:\n",
    "    for item in ax.patches:\n",
    "        height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "        ax.annotate(\n",
    "            '{:.1%}'.format(item.get_height()),\n",
    "            (\n",
    "                item.get_x() + item.get_width() / 2.,\n",
    "                item.get_height()\n",
    "            ),\n",
    "            ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "            textcoords='offset points')\n",
    "    ax.set_ylim(0.5, 1.0)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.gcf().suptitle(\"Classification accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Plot zoomed accuracy\n",
    "accuracy_plt = sns.factorplot(x=\"locus\", y=\"accuracy\", hue=\"method\", data=accuracy_df, kind=\"bar\", size=7)\n",
    "for ax in accuracy_plt.axes.flat:\n",
    "    ax.set_ylim(0.95, 1.0)\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Zoomed classification accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy boxplot\n",
    "plt.figure(figsize=(8,5))\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"accuracy\", hue=\"method\", data=accuracy_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Classification accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le C-Support vector a la meilleure accuracy pour 5 des 6 marqueurs.\n",
    "* Pour HT17 c'est la Logistic Regression, le K-Neighbors et le C-Support Vector qui ont les meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Accuracy summary in dataframe\n",
    "clf_names = [clf.method_name for clf in used_classifiers]\n",
    "\n",
    "cols = {\"locus\": [], \"method\": [], \"accuracy\": [], \"stdev\": []}\n",
    "for locus in models_param:\n",
    "    for idx, clf_name in enumerate(clf_names):\n",
    "        scores = accuracy_df[(accuracy_df.method == clf_name) & (accuracy_df.locus == locus[\"name\"])][\"accuracy\"]\n",
    "        cols[\"locus\"].append(locus[\"name\"])\n",
    "        cols[\"method\"].append(clf_name)\n",
    "        cols[\"accuracy\"].append(scores.mean())\n",
    "        cols[\"stdev\"].append(scores.std())\n",
    "accuracy_summary_df = pd.DataFrame(data=cols)\n",
    "accuracy_summary_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Errors by status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error type\n",
    "plt.figure(figsize=(8,5))\n",
    "error_plt = sns.boxplot(x=\"locus\", y=\"%_error_MSS\", hue=\"method\", data=accuracy_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "error_plt.set_ylim(0, 30)\n",
    "plt.gcf().suptitle(\"% of classification error on MSS\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"%_error_MSI\", hue=\"method\", data=accuracy_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"% of classification error on MSI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Les erreurs sont en majorité faites sur des MSI pris pour des MSS.\n",
    "* Sur les MSS le C-Support Vector est le meilleur ou fait partie des meilleurs classifieurs quelque soit le marqueur.\n",
    "* Sur les MSI le K-Neighbors et le C-Support Vector ont les meilleurs résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy and size of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify data with ShuffleSplit\n",
    "train_size_locus_df = []\n",
    "for locus in models_param:\n",
    "    # Classifiers\n",
    "    used_classifiers = [\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Logistic Regression\", LogisticRegression(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Decision Tree\", DecisionTreeClassifier(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"K-Neighbors\", KNeighborsClassifier(2), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Gaussian Naive Bayes\", GaussianNB(), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"C-Support Vector\", SVC(random_state=init_random_seed, probability=True), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest\", RandomForestClassifier(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest 20\", RandomForestClassifier(random_state=init_random_seed, n_estimators=20), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest 30\", RandomForestClassifier(random_state=init_random_seed, n_estimators=30), data_method_name=method_name\n",
    "        ),\n",
    "        MSINGSClassifier(msings_env, msi_path, locus[\"id\"])\n",
    "    ]\n",
    "\n",
    "    # Select data\n",
    "    whole_labels = []\n",
    "    whole_data_obj = []\n",
    "    for spl in msi_eval:\n",
    "        if spl.loci[locus[\"id\"]].results[method_name].getNbFrag() * 2 >= min_nb_reads:\n",
    "            diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "            if diamic_name in status_by_spl:\n",
    "                status = status_by_spl[diamic_name][locus[\"name\"]]\n",
    "                if status != \"Undetermined\":\n",
    "                    whole_labels.append(status)\n",
    "                    whole_data_obj.append(spl)\n",
    "\n",
    "    # Calculates results\n",
    "    print(\"Start locus {} ({} MSS, {} MSI)\".format(locus[\"name\"], whole_labels.count(\"MSS\"), whole_labels.count(\"MSI\")))\n",
    "    max_size = int(0.65*len(whole_labels))\n",
    "    for train_size in range(25, max_size, 5):\n",
    "        cv = ShuffleSplit(n_splits=200, test_size=(len(whole_labels) - train_size), train_size=train_size, random_state=init_random_seed)\n",
    "        train_size_locus_df.append(\n",
    "            cross_val_score_custom(locus, whole_data_obj, whole_labels, used_classifiers, cv)\n",
    "    )\n",
    "train_size_df = pd.concat(train_size_locus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add train_nb\n",
    "def getTrainSize(row):\n",
    "    return row[\"train_nb_MSS\"] + row[\"train_nb_MSI\"]\n",
    "\n",
    "train_size_df[\"train_nb\"] = train_size_df.apply(getTrainSize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset information\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"train_nb_MSS\", hue=\"method\", data=train_size_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Number of MSS in train dataset\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"train_nb_MSI\", hue=\"method\", data=train_size_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Number of MSI in train dataset\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"test_nb_MSS\", hue=\"method\", data=train_size_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Number of MSS in test dataset\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"test_nb_MSI\", hue=\"method\", data=train_size_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Number of MSI in test dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy summary in dataframe\n",
    "for locus_name in sorted(set(train_size_df[\"locus\"])):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    train_size_plot = sns.pointplot(x=\"train_nb\", y=\"accuracy\", hue=\"method\", data=train_size_df[train_size_df.locus == locus_name])\n",
    "    for item in train_size_plot.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "    plt.gcf().suptitle(\"Accuracy by size of training set for {}\".format(locus_name))\n",
    "    plt.show()\n",
    "\n",
    "for locus_name in sorted(set(train_size_df[\"locus\"])):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    train_size_plot = sns.pointplot(x=\"train_nb\", y=\"accuracy\", hue=\"method\", data=train_size_df[(train_size_df.locus == locus_name) & (train_size_df.method == \"C-Support Vector\")])\n",
    "    for item in train_size_plot.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "    plt.gcf().suptitle(\"Accuracy by size of training set for {}\".format(locus_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for locus_name in sorted(set(train_size_df[\"locus\"])):\n",
    "    test = accuracy_df[accuracy_df.locus == locus_name].apply(getTrainSize, axis=1)\n",
    "    print(locus_name, set(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy and number of reads in sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify data with ShuffleSplit\n",
    "accuracy_reads_locus_df = []\n",
    "for locus in models_param:\n",
    "    for nb_reads_threshold in range(50, 550, 50):\n",
    "        # Classifiers\n",
    "        used_classifiers = [\n",
    "            LocusClassifier(\n",
    "                locus[\"id\"], \"C-Support Vector\", SVC(random_state=init_random_seed, probability=True), data_method_name=method_name\n",
    "            )\n",
    "        ]\n",
    "        # Select data\n",
    "        whole_labels = []\n",
    "        whole_data_obj = []\n",
    "        for spl in msi_eval:\n",
    "            if spl.loci[locus[\"id\"]].results[method_name].getNbFrag() * 2 >= nb_reads_threshold:\n",
    "                diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "                if diamic_name in status_by_spl:\n",
    "                    status = status_by_spl[diamic_name][locus[\"name\"]]\n",
    "                    if status != \"Undetermined\":\n",
    "                        whole_labels.append(status)\n",
    "                        whole_data_obj.append(spl)\n",
    "                        spl.loci[locus[\"id\"]].results[\"model\"] = LocusResPairsCombi(status, data=spl.loci[locus[\"id\"]].results[method_name].data)\n",
    "        # Calculates results\n",
    "        cv = ShuffleSplit(n_splits=200, test_size=0.4, random_state=init_random_seed)\n",
    "        res_df = cross_val_score_custom(locus, whole_data_obj, whole_labels, used_classifiers, cv)\n",
    "        res_df[\"nb_reads\"] = nb_reads_threshold\n",
    "        accuracy_reads_locus_df.append(res_df)\n",
    "accuracy_reads_df = pd.concat(accuracy_reads_locus_df)\n",
    "\n",
    "# Plot accuracy\n",
    "accuracy_reads_plt = sns.factorplot(x=\"locus\", y=\"accuracy\", hue=\"nb_reads\", data=accuracy_reads_df, kind=\"bar\", size=7)\n",
    "for ax in accuracy_reads_plt.axes.flat:\n",
    "    for item in ax.patches:\n",
    "        height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "        ax.annotate(\n",
    "            '{:.1%}'.format(item.get_height()),\n",
    "            (\n",
    "                item.get_x() + item.get_width() / 2.,\n",
    "                item.get_height()\n",
    "            ),\n",
    "            ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "            textcoords='offset points')\n",
    "    ax.set_ylim(0.92, 1.0)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.gcf().suptitle(\"Classification accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Classification accuracy with model used in INCa-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule le taux d'erreur de classification pour chaque classifieur et chaque marqueur en prenant pour base d'entrainement commune le premier modèle utilisé pour l'INCa-v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current model\n",
    "data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "inca_v2_model = MSIReport.parse(os.path.join(data_folder, \"models_TSCA_INCa_V2_2\", \"INCa-V2_models.json\"))\n",
    "\n",
    "# Display samples\n",
    "rows = []\n",
    "for spl in inca_v2_model:\n",
    "    diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "    rows.append({\"sample\": spl.name})\n",
    "    for locus, status in status_by_spl[diamic_name].items():\n",
    "        locus = locus.replace(\"MSI_\", \"\") + \"_status\"\n",
    "        rows[-1][locus] = status\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "models_names = {}\n",
    "cols = {\"sample\": [], \"locus\": [], \"method\": [], \"score\": [], \"expected\": [], \"predicted\": [], \"prediction_status\": []}\n",
    "for locus in models_param:\n",
    "    # Classifiers\n",
    "    used_classifiers = [\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Logistic Regression\", LogisticRegression(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Decision Tree\", DecisionTreeClassifier(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"K-Neighbors\", KNeighborsClassifier(2), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Gaussian Naive Bayes\", GaussianNB(), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"C-Support Vector\", SVC(random_state=init_random_seed, probability=True), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest\", RandomForestClassifier(random_state=init_random_seed), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest 20\", RandomForestClassifier(random_state=init_random_seed, n_estimators=20), data_method_name=method_name\n",
    "        ),\n",
    "        LocusClassifier(\n",
    "            locus[\"id\"], \"Random Forest 30\", RandomForestClassifier(random_state=init_random_seed, n_estimators=30), data_method_name=method_name\n",
    "        )\n",
    "    ]\n",
    "    # sklearn\n",
    "    for idx, classifier in enumerate(used_classifiers):\n",
    "        # Creates model labels\n",
    "        train_data = []\n",
    "        for spl in inca_v2_model:\n",
    "            diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "            locus_res = spl.loci[locus[\"id\"]].results\n",
    "            if \"model\" in locus_res and locus_res[\"model\"].status != \"Undetermined\":\n",
    "                if locus_res[\"model\"].getNbFrag() * 2 >= min_nb_reads:\n",
    "                    models_names[spl.name] = 1\n",
    "                    status = status_by_spl[diamic_name][locus[\"name\"]]\n",
    "                    if status != locus_res[\"model\"].status:\n",
    "                        print(\"The expected status for the sample {} is {} on locus {} but unfortunately the status {} is found in model.\".format(\n",
    "                            spl.name, status, locus[\"name\"], locus_res[\"model\"].status\n",
    "                        ))\n",
    "                    train_data.append(spl)\n",
    "        # Train\n",
    "        classifier.fit(train_data)\n",
    "        # Classify\n",
    "        for spl in msi_eval:\n",
    "            diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "            if diamic_name in status_by_spl and spl.name not in models_names:\n",
    "                if spl.loci[locus[\"id\"]].results[method_name].getNbFrag() * 2 >= min_nb_reads:\n",
    "                    expected_status = status_by_spl[diamic_name][locus[\"name\"]]\n",
    "                    if expected_status != \"Undetermined\":\n",
    "                        predicted_status = classifier.predict([spl])[0]\n",
    "                        cols[\"sample\"].append(spl.name.replace(\"_L001\", \"\"))\n",
    "                        cols[\"locus\"].append(locus[\"name\"])\n",
    "                        cols[\"method\"].append(classifier.method_name)\n",
    "                        cols[\"score\"].append(classifier._get_scores([predicted_status])[0])\n",
    "                        cols[\"expected\"].append(expected_status)\n",
    "                        cols[\"predicted\"].append(predicted_status)\n",
    "                        if predicted_status != \"Undetermined\":\n",
    "                            cols[\"prediction_status\"].append(expected_status == predicted_status)\n",
    "                        else:\n",
    "                            cols[\"prediction_status\"].append(\"NA\")\n",
    "    # MIAmS\n",
    "    for idx, classifier_name in enumerate([\"MSINGS\", method_name]):\n",
    "        for spl in msi_eval:\n",
    "            diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "            if diamic_name in status_by_spl and spl.name not in models_names:\n",
    "                if spl.loci[locus[\"id\"]].results[method_name].getNbFrag() * 2 >= min_nb_reads:\n",
    "                    expected_status = status_by_spl[diamic_name][locus[\"name\"]]\n",
    "                    if expected_status != \"Undetermined\":\n",
    "                        predicted_status = spl.loci[locus[\"id\"]].results[classifier_name].status\n",
    "                        cols[\"sample\"].append(spl.name.replace(\"_L001\", \"\"))\n",
    "                        cols[\"locus\"].append(locus[\"name\"])\n",
    "                        cols[\"method\"].append(classifier_name)\n",
    "                        cols[\"score\"].append(spl.loci[locus[\"id\"]].results[classifier_name].score)\n",
    "                        cols[\"expected\"].append(expected_status)\n",
    "                        cols[\"predicted\"].append(predicted_status)\n",
    "                        if predicted_status != \"Undetermined\":\n",
    "                            cols[\"prediction_status\"].append(expected_status == predicted_status)\n",
    "                        else:\n",
    "                            cols[\"prediction_status\"].append(\"NA\")\n",
    "eval_df = pd.DataFrame(data=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation\n",
    "plt_whole = sns.countplot(x=\"prediction_status\", hue=\"method\", data=eval_df, order=[True, False, \"NA\"])\n",
    "plt_whole.set_title(\"Locus classification\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt_by_locus = sns.factorplot(x=\"prediction_status\", hue=\"method\", col=\"locus\", data=eval_df, order=[True, False, \"NA\"], kind=\"count\")\n",
    "for ax in plt_by_locus.axes.flat:\n",
    "    locus_name = ax.get_title().split(\" \")[-1]\n",
    "    total_count = len(eval_df[(eval_df[\"locus\"] == locus_name) & (eval_df[\"method\"] == \"Logistic Regression\")])\n",
    "    for item in ax.patches:\n",
    "        height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "        ax.annotate(\n",
    "            '{:.1%}'.format(item.get_height()/total_count),\n",
    "            (\n",
    "                item.get_x() + item.get_width() / 2.,\n",
    "                item.get_height()\n",
    "            ),\n",
    "            ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "            textcoords='offset points')\n",
    "plt.subplots_adjust(top=0.8)\n",
    "plt.gcf().suptitle(\"Locus classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classification avec Logistic Regression, K-Neighbors et C-Support Vector ont les plus faibles taux d'erreur sur l'ensemble des marqueurs.\n",
    "\n",
    "Les bonnes performances de classifieurs pourtant très simples comme le Logistic Regression et K-Neighbors s'expliquent par le peu de différence intra-statut et la forte différence inter-statut dans les profils des échantillons fournis. L'ensemble de test ne contient pas de cas difficile en terme de % de séquences dans le deuxième pic ou de distance entre les pics.\n",
    "\n",
    "mSINGS est toujours moins bon que ces trois classieurs (excepté sur BAT25 en comparaison avec la régression logistique). Il a le taux d'erreur le plus élevé sur BAT26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Prediction probabilities evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche à savoir si les classifieurs fournissent une probabilité de prédiction utilisable. C'est à dire un score qui permet de distinguer 2 groupes:\n",
    "  * Un groupe contenant la très grande majorité des fausses prédiction (idéalement toutes) et quelques bonnes prédictions.\n",
    "  * Un groupe dont on est sûr de la prédiction (peu voire pas de mauvaise prédiction et un maximum de bonnes prédictions).\n",
    "\n",
    "Un classifieur qui à un très faible taux d'erreur mais un score de probabilité ne permettant pas d'éliminer les fausses prédictions ne sera pas forcément préféré à un classifieur avec un faible taux d'erreur mais pour lequel on pourra faire confiance au score de prédiction. Dans le premier cas on aura jamais confiance dans la prédiction et on sera obligé de contrôler tous les cas. Dans le deuxième cas on fera confiance dans la classification des très haut score et on reverra manuellement tout les autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate prediction probability\n",
    "clf_names = [\"Logistic Regression\", \"Decision Tree\", \"K-Neighbors\", \"C-Support Vector\", \"Gaussian Naive Bayes\", \"Random Forest\", \"Random Forest 20\", \"Random Forest 30\", \"MSINGS\", method_name]\n",
    "for clf_name in clf_names:\n",
    "    sns.factorplot(\n",
    "        x=\"prediction_status\",\n",
    "        y=\"score\",\n",
    "        col=\"locus\",\n",
    "        order=[True, False],\n",
    "        data=eval_df[eval_df.method == clf_name],\n",
    "        kind=\"box\"\n",
    "    )\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.gcf().suptitle(clf_name) # can also get the figure from plt.gcf()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les probabilités associées à la prédiction du Random Forest et dans une moindre mesure du C-Support Vector semblent permettre de distinguer la très grande majorité des mauvaises classification (au prix bien entendu de perdre une part importante de bonne classification).\n",
    "\n",
    "Attention : Dans le jeux de données le nombre de fausses classification n'était pas très important et il est donc difficile de tirer des conclusions indiscutable. on travaillera sur des tendance que l'on confirmera en simulation ou avec plus de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate prediction probability usage\n",
    "prediction_score_threshold = 0.9\n",
    "methods = [\"Random Forest\", \"C-Support Vector\"]\n",
    "for clf_name in methods:\n",
    "    labels = [\"Score\", \"Valid prediction\", \"Invalid prediction\"]\n",
    "    values = [\n",
    "        (\n",
    "            \">= {}\".format(prediction_score_threshold),\n",
    "            len(eval_df[(eval_df.prediction_status == True) & (eval_df.method == clf_name) & (eval_df.score >= prediction_score_threshold)]),\n",
    "            len(eval_df[(eval_df.prediction_status == False) & (eval_df.method == clf_name) & (eval_df.score >= prediction_score_threshold)])\n",
    "        ),\n",
    "        (\n",
    "            \"< {}\".format(prediction_score_threshold),\n",
    "            len(eval_df[(eval_df.prediction_status == True) & (eval_df.method == clf_name) & (eval_df.score < prediction_score_threshold)]),\n",
    "            len(eval_df[(eval_df.prediction_status == False) & (eval_df.method == clf_name) & (eval_df.score < prediction_score_threshold)])\n",
    "        )\n",
    "    ]\n",
    "    print(\"{}\\n\".format(clf_name))\n",
    "    print(pd.DataFrame.from_records(values, columns=labels))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec une probabilité de prédiction >= {{threshold}} on élimine quasiment toutes les fausses prédictions et on obtiens un nombre suffisant de classification automatique.\n",
    "\n",
    "On constate que si on se concentre uniquement sur les prédiction avec une forte confiance le Random forest deviens meilleur que le C-Support Vector.\n",
    "\n",
    "Au vu de ces chiffres on pourrait utiliser le Random Forest pour donner les prédictions de confiance et le C-Support Vector pour aider à la décision sur la revue manuelle des autres cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List errors with high prediction score\n",
    "eval_df[(eval_df.prediction_status == False) & eval_df.method.isin(methods) & (eval_df.score >= prediction_score_threshold)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation at sample level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Status based on pentaplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify samples with same models in all methods and votation of pentaplex\n",
    "nb_spl = 0\n",
    "cols = {\"sample\": [], \"method\": [], \"nb_loci\": [], \"nb_MSS\": [], \"nb_MSI\": [], \"score\": [], \"expected\": [], \"predicted\": [], \"prediction_status\": []}\n",
    "for clf_name in clf_names:\n",
    "    filtered_df = eval_df[(eval_df.method == clf_name) & (eval_df.locus != \"MSI_HT17\")]\n",
    "    res_by_spl = {}\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        if row[\"sample\"] not in res_by_spl:\n",
    "            res_by_spl[row[\"sample\"]] = {}\n",
    "        res_by_spl[row[\"sample\"]][row[\"locus\"]] = row[\"predicted\"]\n",
    "    for spl, loci in res_by_spl.items():\n",
    "        diamic_name = spl.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "        status_list = list(loci.values())\n",
    "        nb_MSS = status_list.count(\"MSS\")\n",
    "        nb_MSI = status_list.count(\"MSI\")\n",
    "        expected_spl_status = status_by_spl[diamic_name][\"sample\"]\n",
    "        predicted_spl_status = \"Undetermined\"\n",
    "        if nb_MSS > nb_MSI and nb_MSS >= 3:\n",
    "            predicted_spl_status = \"MSS\"\n",
    "        elif nb_MSI > nb_MSS and nb_MSI >= 3:\n",
    "            predicted_spl_status = \"MSI\"\n",
    "        cols[\"sample\"].append(spl)\n",
    "        cols[\"method\"].append(clf_name)\n",
    "        cols[\"score\"].append(np.nan)\n",
    "        cols[\"nb_loci\"].append(nb_MSS + nb_MSI)\n",
    "        cols[\"nb_MSS\"].append(nb_MSS)\n",
    "        cols[\"nb_MSI\"].append(nb_MSI)\n",
    "        cols[\"expected\"].append(expected_spl_status)\n",
    "        cols[\"predicted\"].append(predicted_spl_status)\n",
    "        if expected_spl_status == \"Undetermined\":\n",
    "            cols[\"prediction_status\"].append(\"DB_Undetermined\")\n",
    "        elif predicted_spl_status != \"Undetermined\":\n",
    "            cols[\"prediction_status\"].append(expected_spl_status == predicted_spl_status)\n",
    "        else:\n",
    "            cols[\"prediction_status\"].append(\"NA\")\n",
    "    nb_spl = len(res_by_spl)\n",
    "spl_pentaplex_df = pd.DataFrame(data=cols)\n",
    "\n",
    "# Plot evaluation\n",
    "ax = sns.countplot(x=\"prediction_status\", hue=\"method\", data=spl_pentaplex_df, order=[True, False, \"NA\", \"DB_Undetermined\"])\n",
    "ax.set_ylim(0, 250)\n",
    "for item in ax.patches:\n",
    "    height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "    ax.annotate(\n",
    "        '{:.1%}'.format(item.get_height()/nb_spl),\n",
    "        (\n",
    "            item.get_x() + item.get_width() / 2.,\n",
    "            item.get_height()\n",
    "        ),\n",
    "        ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "        textcoords='offset points')\n",
    "ax.set_title(\"Samples stability classification based on pentaplex\")\n",
    "plt.show()\n",
    "\n",
    "# List or errors\n",
    "spl_pentaplex_df[(spl_pentaplex_df.prediction_status == False) & (spl_pentaplex_df.method == \"C-Support Vector\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les taux d'erreur avec le vote sur les 5 marqueurs du pentaplex sont faibles : de 1 à 2 % pour les classifieurs déjà retenus K-neighbors et C-Support Vector.\n",
    "\n",
    "Une partie faible mais significative des échantillons n'a pas de statut (4 à 5 % de NA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Status based on HT17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for HT17 and compare to sample\n",
    "cols = {\"sample\": [], \"method\": [], \"score\": [], \"expected\": [], \"predicted\": [], \"prediction_status\": []}\n",
    "for curr_method in clf_names:\n",
    "    filtered_df = eval_df[(eval_df.method == curr_method) & (eval_df.locus == \"MSI_HT17\")]\n",
    "    predict_by_spl = {}\n",
    "    score_by_spl = {}\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        predict_by_spl[row[\"sample\"]] = row[\"predicted\"]\n",
    "        score_by_spl[row[\"sample\"]] = row[\"score\"]\n",
    "    for spl, predicted_ht17 in predict_by_spl.items():\n",
    "        diamic_name = spl.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "        expected_spl_status = status_by_spl[diamic_name][\"sample\"]\n",
    "        cols[\"sample\"].append(spl)\n",
    "        cols[\"method\"].append(curr_method)\n",
    "        cols[\"score\"].append(score_by_spl[spl])\n",
    "        cols[\"expected\"].append(expected_spl_status)\n",
    "        cols[\"predicted\"].append(predicted_ht17)\n",
    "        if expected_spl_status == \"Undetermined\":\n",
    "            cols[\"prediction_status\"].append(\"DB_Undetermined\")\n",
    "        elif predicted_ht17 != \"Undetermined\":\n",
    "            cols[\"prediction_status\"].append(expected_spl_status == predicted_ht17)\n",
    "        else:\n",
    "            cols[\"prediction_status\"].append(\"NA\")\n",
    "spl_ht17_df = pd.DataFrame(data=cols)\n",
    "\n",
    "# Plot evaluation\n",
    "nb_samples = len(spl_ht17_df[spl_ht17_df.method == \"Random Forest\"])\n",
    "ax = sns.countplot(x=\"prediction_status\", hue=\"method\", data=spl_ht17_df, order=[True, False, \"NA\", \"DB_Undetermined\"])\n",
    "ax.set_ylim(0, 250)\n",
    "for item in ax.patches:\n",
    "    height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "    ax.annotate(\n",
    "        '{:.1%}'.format(height/nb_samples),\n",
    "        (\n",
    "            item.get_x() + item.get_width() / 2.,\n",
    "            item.get_height()\n",
    "        ),\n",
    "        ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "        textcoords='offset points')\n",
    "ax.set_title(\"Samples stability classification based on HT17\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()\n",
    "\n",
    "# List or errors\n",
    "spl_ht17_df[(spl_ht17_df.prediction_status == False) & (spl_ht17_df.method == \"C-Support Vector\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classifieurs basés Logistic Regression, K-neighbors et C-Support Vector ont de bien meilleur résultats que mSINGS: 2.1% d'erreur contre 12.9%.\n",
    "\n",
    "Bien que le taux d'erreur soit plus fort qu'avec le pentaplex (2.1% contre 1%), le taux de correctement classifiés est supérieur car on a moins de problème avec ce seul marqueur (97.9% contre 94.5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by locus and compare to sample\n",
    "for curr_model in models_param:\n",
    "    cols = {\"sample\": [], \"method\": [], \"expected\": [], \"predicted\": [], \"prediction_status\": []}\n",
    "    for curr_method in clf_names:\n",
    "        filtered_df = eval_df[(eval_df.method == curr_method) & (eval_df.locus == curr_model[\"name\"])]\n",
    "        predict_by_spl = {}\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            predict_by_spl[row[\"sample\"]] = row[\"predicted\"]\n",
    "        for spl, predicted_locus in predict_by_spl.items():\n",
    "            diamic_name = spl.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "            expected_spl_status = status_by_spl[diamic_name][\"sample\"]\n",
    "            if expected_spl_status != \"Undetermined\":\n",
    "                cols[\"sample\"].append(spl)\n",
    "                cols[\"method\"].append(curr_method)\n",
    "                cols[\"expected\"].append(expected_spl_status)\n",
    "                cols[\"predicted\"].append(predicted_locus)\n",
    "                if expected_spl_status == \"Undetermined\":\n",
    "                    cols[\"prediction_status\"].append(\"DB_Undetermined\")\n",
    "                elif predicted_locus != \"Undetermined\":\n",
    "                    cols[\"prediction_status\"].append(expected_spl_status == predicted_locus)\n",
    "                else:\n",
    "                    cols[\"prediction_status\"].append(\"NA\")\n",
    "    locus_predict_spl_df = pd.DataFrame(data=cols)\n",
    "\n",
    "    # Plot evaluation\n",
    "    nb_samples = len(locus_predict_spl_df[locus_predict_spl_df.method == \"Random Forest\"])\n",
    "    ax = sns.countplot(x=\"prediction_status\", hue=\"method\", data=locus_predict_spl_df, order=[True, False, \"NA\", \"DB_Undetermined\"])\n",
    "    ax.set_ylim(0, 250)\n",
    "    for item in ax.patches:\n",
    "        height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "        ax.annotate(\n",
    "            '{:.1%}'.format(item.get_height()/nb_samples),\n",
    "            (\n",
    "                item.get_x() + item.get_width() / 2.,\n",
    "                item.get_height()\n",
    "            ),\n",
    "            ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "            textcoords='offset points')\n",
    "    ax.set_title(\"Samples stability classification based on {}\".format(curr_model[\"name\"]))\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur les cas évalués quand NR27 est présent (142 librairies) il est meilleur que HT17 (194 librairies) pour donner le status de l'échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors with C-Support Vector\n",
    "spl_ht17_df[(spl_ht17_df.method == \"C-Support Vector\") & (spl_ht17_df.prediction_status == False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les profils de 17T044302_S22 et 17T005560_S47 correspondent parfaitement à du MSS.\n",
    "\n",
    "Les deux autres ont un score plus réduit ce qui va dans le sens d'une erreur avec une faible confiance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors with Random Forest\n",
    "spl_ht17_df[(spl_ht17_df.method == \"Random Forest\") & (spl_ht17_df.prediction_status == False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excepté pour 17T044302_S22 et 17T005560_S47 le score montre que l'on pourrait mettre une alerte sur les erreurs de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## combiner pentaplex + HT17 pour voir si erreur diminue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples database information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont de 3 sources:\n",
    " * les résultats extrait de diamic pour l'électrophorèse sur le pentaplex : BAT25, BAT26, NR21, NR22, NR24. Ces résultats ont été complétés pour les échantillons passés en NGS par une révision manuelle : NR27 et HT17.\n",
    " * les résultats révisé manuellement des échantillons de routine à prioris MSS passés en NGS INCa-V2 pour valider la partie mutation du panel :  HT17 NR27\n",
    " * les résultats des échantillons du run NGS de montpellier 180430 révisé manuellement: BAT25, BAT26, NR21, NR22, NR27 et HT17. Le run contenait des MSS connu, des MSI connu et des statut indéterminé (passé pour les mutations).\n",
    "\n",
    "Les révisions manuelles faite via NGS s'appuient sur les profils des combinaisons de paire en tenant compte des réplicats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load status by locus and compare to the sample status\n",
    "cols = {\"sample\": [], \"in_ngs\": [], \"sample_status\": [], \"locus\": [], \"classification_status\": []}\n",
    "for spl_name, status in status_by_spl.items():\n",
    "    in_ngs = False\n",
    "    if status[\"MSI_HT17\"] != \"\" and status[\"sample\"]:\n",
    "        in_ngs = True\n",
    "    for locus in [\"MSI_BAT25\", \"MSI_BAT26\", \"MSI_NR21\", \"MSI_NR22\", \"MSI_NR24\", \"MSI_NR27\", \"MSI_HT17\"]:\n",
    "        cols[\"sample\"].append(spl_name)\n",
    "        cols[\"in_ngs\"].append(in_ngs)\n",
    "        cols[\"sample_status\"].append(status[\"sample\"])\n",
    "        cols[\"locus\"].append(locus)\n",
    "        validity = \"NA\"\n",
    "        if status[\"sample\"] != \"Undetermined\" and status[locus] != \"\" and status[locus] != \"Undetermined\":\n",
    "            validity = (status[\"sample\"] == status[locus])\n",
    "        cols[\"classification_status\"].append(validity)\n",
    "locus_status_df = pd.DataFrame(data=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of samples well classified if the reference is only one locus\n",
    "nb_spl = len(locus_status_df[locus_status_df.locus == \"MSI_HT17\"])\n",
    "ax = sns.countplot(x=\"classification_status\", hue=\"locus\", data=locus_status_df, order=[True, False, \"NA\"])\n",
    "ax.set_ylim(0, nb_spl + nb_spl * 0.2)\n",
    "for item in ax.patches:\n",
    "    height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "    ax.annotate(\n",
    "        '{:.1%}'.format(item.get_height()/nb_spl),\n",
    "        (\n",
    "            item.get_x() + item.get_width() / 2.,\n",
    "            item.get_height()\n",
    "        ),\n",
    "        ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "        textcoords='offset points')\n",
    "ax.set_title(\"Number of samples well classified by only one locus\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le taux d'erreur si l'on se fiait à uniquement un marqueur est inférieur à 2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of samples well classified if the reference is only one locus in samples selected for NGS\n",
    "ngs_locus_status_df = locus_status_df[locus_status_df.in_ngs == True]\n",
    "nb_spl = len(ngs_locus_status_df[ngs_locus_status_df.locus == \"MSI_HT17\"])\n",
    "ax = sns.countplot(x=\"classification_status\", hue=\"locus\", data=ngs_locus_status_df, order=[True, False, \"NA\"])\n",
    "ax.set_ylim(0, nb_spl + nb_spl * 0.2)\n",
    "for item in ax.patches:\n",
    "    height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "    ax.annotate(\n",
    "        '{:.1%}'.format(item.get_height()/nb_spl),\n",
    "        (\n",
    "            item.get_x() + item.get_width() / 2.,\n",
    "            item.get_height()\n",
    "        ),\n",
    "        ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "        textcoords='offset points')\n",
    "ax.set_title(\"Number of samples selected for NGS well classified by only one locus\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "locus_status_df[(locus_status_df.classification_status == False) & (locus_status_df.locus == \"MSI_HT17\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur les cas utilisés pour l'analyse HT17 à l'un des plus fort pouvoir prédictif. Il reste cependant au niveau de BAT26 et NR22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Create simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "nb_val = 10000\n",
    "nb_repeat = 20\n",
    "\n",
    "for curr_model in models_param:\n",
    "    center = curr_model[\"mean\"]\n",
    "    std_dev = [curr_model[\"std_dev\"] - 0.1, curr_model[\"std_dev\"], curr_model[\"std_dev\"] + 0.1]\n",
    "\n",
    "    np.random.seed(init_random_seed)\n",
    "    distances = range(1, 10)\n",
    "    tum_frequencies = range(0, 100, 5)\n",
    "    nb_spl = nb_repeat * len(distances) * len(tum_frequencies)\n",
    "    random_seeds = rd.sample(range(1, 2**32-1), k=nb_spl + 1)\n",
    "    curr_idx = 0\n",
    "    for curr_generator in range(nb_repeat):\n",
    "        for curr_af in tum_frequencies:   \n",
    "            for curr_distance in distances:\n",
    "                nb_tumor = int((curr_af/100)*nb_val)\n",
    "                nb_healthy = nb_val - nb_tumor\n",
    "                series_params = [\n",
    "                    {\"random_seed\": random_seeds[curr_idx], \"center\": center, \"std_dev\": rd.choice(std_dev), \"nb_val\": nb_healthy},\n",
    "                    {\"random_seed\": random_seeds[curr_idx] + 1, \"center\": center - curr_distance, \"std_dev\": rd.choice(std_dev), \"nb_val\": nb_tumor},\n",
    "                ]\n",
    "\n",
    "                # Create sub-series\n",
    "                series = list()\n",
    "                for curr_params in series_params:\n",
    "                    if \"random_seed\" in curr_params:\n",
    "                        np.random.seed(curr_params[\"random_seed\"])\n",
    "                    series.append(\n",
    "                        np.random.normal(curr_params[\"center\"], curr_params[\"std_dev\"], curr_params[\"nb_val\"]).astype(int)\n",
    "                    )\n",
    "\n",
    "                # Next random seed\n",
    "                curr_idx = curr_idx + 1\n",
    "\n",
    "                # Add loci to evaluated\n",
    "                spl_name = \"tum_{}_on_{}\".format(curr_af, center - curr_distance)\n",
    "                merged_series = mergeSeries(series)\n",
    "                loci_res = LocusResPairsCombi(None, None, {\"nb_by_length\": getStacked(merged_series)})\n",
    "                loci = MSILocus(curr_model[\"id\"], curr_model[\"name\"], {method_name: loci_res})\n",
    "                curr_model[\"simulated_data\"].append(\n",
    "                    MSISample(spl_name, {curr_model[\"id\"]: loci})\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store simulation data\n",
    "simu_folder = os.path.join(os.getcwd(), \"data\", \"simulation\")\n",
    "for curr_model in models_param:\n",
    "    MSIReport.write(\n",
    "        curr_model[\"simulated_data\"],\n",
    "        os.path.join(simu_folder, curr_model[\"name\"] + \".json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Classify simulated samples with learn on simulated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_name = [\"Logistic Regression\", \"Decision Tree\", \"K-Neighbors\", \"Gaussian Naive Bayes\", \"C-Support Vector\", \"Random Forest\", \"Random Forest 20\", \"Random Forest 30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify simulated samples with learn on simulated samples\n",
    "cols = {\n",
    "    \"sample\": [],\n",
    "    \"locus\": [],\n",
    "    \"method\": [],\n",
    "    \"training_idx\": [],\n",
    "    \"score\": [],\n",
    "    \"expected\": [],\n",
    "    \"predicted\": [],\n",
    "    \"prediction_status\": []\n",
    "}\n",
    "\n",
    "summary_cols = {\n",
    "    \"locus\": [],\n",
    "    \"method\": [],\n",
    "    \"accuracy\": []\n",
    "}\n",
    "\n",
    "for curr_model in models_param:\n",
    "    # Creates labels\n",
    "    whole_name = []\n",
    "    whole_data = []\n",
    "    whole_labels = []\n",
    "    for spl in curr_model[\"simulated_data\"]:\n",
    "        if spl.loci[curr_model[\"id\"]].results[method_name].getNbFrag() * 2 >= min_nb_reads:\n",
    "            series = spl.loci[curr_model[\"id\"]].results[method_name].getDensePrct(50, 200)\n",
    "            status = \"MSI\"\n",
    "            if spl.name.startswith(\"tum_0_\") or spl.name.startswith(\"tum_5_\") or \\\n",
    "               spl.name.endswith(\"_on_{}\".format(int(curr_model[\"mean\"]))) or \\\n",
    "               spl.name.endswith(\"_on_{}\".format(int(curr_model[\"mean\"]) - 1)):\n",
    "                status = \"MSS\"\n",
    "            whole_name.append(spl.name)\n",
    "            whole_data.append(series)\n",
    "            whole_labels.append(status)\n",
    "    \n",
    "    # Calculates results\n",
    "    training_idx = 0\n",
    "    cv = ShuffleSplit(n_splits=200, test_size=0.25, random_state=init_random_seed)\n",
    "    print(\"{}: {} MSS, {} MSI, {} datasets\".format(curr_model[\"name\"], whole_labels.count(\"MSS\"), whole_labels.count(\"MSI\"), cv.get_n_splits()))\n",
    "    for train_idx, test_idx in cv.split(whole_data, groups=whole_labels):\n",
    "        training_idx += 1\n",
    "        # Split dataset in train and test sub-datasets\n",
    "        train = {\"names\": [], \"data\": [], \"labels\": []}\n",
    "        test = {\"names\": [], \"data\": [], \"labels\": []}\n",
    "        for data_idx, data_series in enumerate(whole_data):\n",
    "            if data_idx in train_idx:\n",
    "                train[\"names\"].append(whole_name[data_idx])\n",
    "                train[\"data\"].append(whole_data[data_idx])\n",
    "                train[\"labels\"].append(whole_labels[data_idx])\n",
    "            elif data_idx in test_idx:\n",
    "                test[\"names\"].append(whole_name[data_idx])\n",
    "                test[\"data\"].append(whole_data[data_idx])\n",
    "                test[\"labels\"].append(whole_labels[data_idx])\n",
    "        # Classifiers\n",
    "        learning_methods = [\n",
    "            {\"name\": \"Logistic Regression\", \"clf\": LogisticRegression(random_state=init_random_seed)},\n",
    "            {\"name\": \"Decision Tree\", \"clf\": DecisionTreeClassifier(random_state=init_random_seed)},\n",
    "            {\"name\": \"K-Neighbors\", \"clf\": KNeighborsClassifier(2)},\n",
    "            {\"name\": \"Gaussian Naive Bayes\", \"clf\": GaussianNB()},\n",
    "            {\"name\": \"C-Support Vector\", \"clf\": SVC(random_state=init_random_seed, probability=True)},\n",
    "            {\"name\": \"Random Forest\", \"clf\": RandomForestClassifier(random_state=init_random_seed)},\n",
    "            {\"name\": \"Random Forest 20\", \"clf\": RandomForestClassifier(random_state=init_random_seed, n_estimators=20)},\n",
    "            {\"name\": \"Random Forest 30\", \"clf\": RandomForestClassifier(random_state=init_random_seed, n_estimators=30)},\n",
    "        ]\n",
    "        # Use classifiers\n",
    "        for idx, curr_method in enumerate(learning_methods):\n",
    "            # Train\n",
    "            clf = curr_method[\"clf\"].fit(train[\"data\"], train[\"labels\"])\n",
    "            # Classify\n",
    "            predictions = clf.predict(test[\"data\"])\n",
    "            probabilities = clf.predict_proba(test[\"data\"])\n",
    "            nb_true = 0\n",
    "            nb_false = 0\n",
    "            for pred_idx, pred_label in enumerate(predictions):\n",
    "                cols[\"sample\"].append(test[\"names\"][pred_idx])\n",
    "                cols[\"locus\"].append(curr_model[\"name\"])\n",
    "                cols[\"method\"].append(curr_method[\"name\"])\n",
    "                cols[\"training_idx\"].append(training_idx)\n",
    "                status_idx = np.nonzero(clf.classes_ == pred_label)[0][0]\n",
    "                cols[\"score\"].append(probabilities[pred_idx][status_idx])\n",
    "                cols[\"expected\"].append(test[\"labels\"][pred_idx])\n",
    "                cols[\"predicted\"].append(pred_label)\n",
    "                cols[\"prediction_status\"].append(test[\"labels\"][pred_idx] == pred_label)\n",
    "                if test[\"labels\"][pred_idx] == pred_label:\n",
    "                    nb_true += 1\n",
    "                else:\n",
    "                    nb_false += 1\n",
    "            summary_cols[\"locus\"].append(curr_model[\"name\"])\n",
    "            summary_cols[\"method\"].append(curr_method[\"name\"])\n",
    "            summary_cols[\"accuracy\"].append(nb_true/(nb_true + nb_false))\n",
    "        \n",
    "simuTrain_simuData_df = pd.DataFrame(data=cols)\n",
    "summary_simuTrain_simuData_df = pd.DataFrame(data=summary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot zoomed accuracy\n",
    "accuracy_plt = sns.factorplot(x=\"locus\", y=\"accuracy\", hue=\"method\", data=summary_simuTrain_simuData_df, kind=\"bar\", size=6)\n",
    "for ax in accuracy_plt.axes.flat:\n",
    "    ax.set_ylim(0.95, 1.0)\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Zoomed classification accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy boxplot\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"accuracy\", hue=\"method\", data=summary_simuTrain_simuData_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Classification accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy boxplot\n",
    "accuracy_plt = sns.boxplot(x=\"locus\", y=\"accuracy\", hue=\"method\", data=summary_simuTrain_simuData_df, medianprops=dict(linewidth=2, color='firebrick'))\n",
    "accuracy_plt.set_ylim(0.96, 1.0)\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.gcf().suptitle(\"Zoomed classification accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate prediction probability\n",
    "for curr_method in classifiers_name:\n",
    "    sns.factorplot(\n",
    "        x=\"prediction_status\",\n",
    "        y=\"score\",\n",
    "        col=\"locus\",\n",
    "        order=[True, False],\n",
    "        data=simuTrain_simuData_df[simuTrain_simuData_df.method == curr_method],\n",
    "        kind=\"box\"\n",
    "    )\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.gcf().suptitle(\"Prediction probability by prediction status with {}\".format(curr_method))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forte utilité de la probabilité de prédction du Random Forest et dans une moindre mesure du C-Support Vector sont confirmée.\n",
    "\n",
    "On confirme également que plus on augmente le nombre d'abre du Random Forest plus l'accuracy augmente mais moins la probabilité de la prédiction est utilisable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Classify real samples with learn on simulated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classify and compare to the expected\n",
    "cols = {\"sample\": [], \"locus\": [], \"method\": [], \"expected\": [], \"predicted\": [], \"prediction_status\": []}\n",
    "for curr_model in models_param:\n",
    "    for idx, curr_method in enumerate(learning_methods):\n",
    "        # Creates simulation labels\n",
    "        whole_data = []\n",
    "        whole_labels = []\n",
    "        for spl in curr_model[\"simulated_data\"]:\n",
    "            if spl.loci[curr_model[\"id\"]].results[method_name].getNbFrag() * 2 >= min_nb_reads:\n",
    "                series = spl.loci[curr_model[\"id\"]].results[method_name].getDensePrct(50, 200)\n",
    "                status = \"MSI\"\n",
    "                if spl.name.startswith(\"tum_0_\") or spl.name.startswith(\"tum_5_\") or \\\n",
    "                   spl.name.endswith(\"_on_{}\".format(int(curr_model[\"mean\"]))) or \\\n",
    "                   spl.name.endswith(\"_on_{}\".format(int(curr_model[\"mean\"]) - 1)):\n",
    "                    status = \"MSS\"\n",
    "                whole_data.append(series)\n",
    "                whole_labels.append(status)\n",
    "        # Train\n",
    "        clf = curr_method[\"clf\"].fit(whole_data, whole_labels)\n",
    "        # Classify\n",
    "        for spl in msi_eval:\n",
    "            diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "            if diamic_name in status_by_spl:\n",
    "                if spl.loci[curr_model[\"id\"]].results[method_name].getNbFrag() * 2 >= min_nb_reads:\n",
    "                    series = spl.loci[curr_model[\"id\"]].results[method_name].getDensePrct(50, 200)\n",
    "                    predicted_status = clf.predict([series])[0]\n",
    "                    expected_status = status_by_spl[diamic_name][curr_model[\"name\"]]\n",
    "                    cols[\"sample\"].append(spl.name.replace(\"_L001\", \"\"))\n",
    "                    cols[\"locus\"].append(curr_model[\"name\"])\n",
    "                    cols[\"method\"].append(curr_method['name'])\n",
    "                    cols[\"expected\"].append(expected_status)\n",
    "                    cols[\"predicted\"].append(predicted_status)\n",
    "                    if expected_status != \"Undetermined\" and predicted_status != \"Undetermined\":\n",
    "                        cols[\"prediction_status\"].append(expected_status == predicted_status)\n",
    "                    else:\n",
    "                        cols[\"prediction_status\"].append(\"NA\")\n",
    "simuTrain_realData_df = pd.DataFrame(data=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation\n",
    "plt_whole = sns.countplot(x=\"prediction_status\", hue=\"method\", data=simuTrain_realData_df, order=[True, False, \"NA\"])\n",
    "plt_whole.set_title(\"Locus classification\")\n",
    "\n",
    "plt_by_locus = sns.factorplot(x=\"prediction_status\", hue=\"method\", col=\"locus\", data=simuTrain_realData_df, order=[True, False, \"NA\"], kind=\"count\")\n",
    "for ax in plt_by_locus.axes.flat:\n",
    "    locus_name = ax.get_title().split(\" \")[-1]\n",
    "    total_count = len(simuTrain_realData_df[(simuTrain_realData_df.locus == locus_name) & (simuTrain_realData_df.method == \"Random Forest\")])\n",
    "    for item in ax.patches:\n",
    "        height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "        ax.annotate(\n",
    "            '{:.1%}'.format(item.get_height()/total_count),\n",
    "            (\n",
    "                item.get_x() + item.get_width() / 2.,\n",
    "                item.get_height()\n",
    "            ),\n",
    "            ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "            textcoords='offset points')\n",
    "plt.subplots_adjust(top=0.8)\n",
    "plt.gcf().suptitle(\"Locus classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = eval_df[eval_df[\"prediction_status\"] == False]\n",
    "g = sns.factorplot(x=\"sample\", hue=\"locus\", data=error_df, palette=\"muted\", kind='count', aspect=3)\n",
    "g.set_xticklabels(rotation=90)\n",
    "g.set_ylabels(\"Nb_methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis without machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter, find_peaks_cwt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "smooth_sigma = 1.1\n",
    "\n",
    "test_curves = [\n",
    "    {\"data\": [0, 100, 0, 0, 0, 0], \"nb_peaks\": 1},\n",
    "    {\"data\": [100, 0, 0, 0, 0], \"nb_peaks\": 1},\n",
    "    {\"data\": [0, 100, 80, 90, 70, 0], \"nb_peaks\": 1},\n",
    "    {\"data\": [0, 100, 80, 75, 70, 65, 60, 55, 50, 45, 40, 35, 30, 25, 20, 15, 10, 5, 0], \"nb_peaks\": 1},\n",
    "    {\"data\": [0, 100, 0, 40, 0], \"nb_peaks\": 1},\n",
    "    {\"data\": [0, 100, 0, 0, 40, 0], \"nb_peaks\": 2},\n",
    "    {\"data\": [0, 30, 80, 100, 0, 0, 20, 40, 20, 0], \"nb_peaks\": 2},\n",
    "    {\"data\": [0, 100, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 17, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 0], \"nb_peaks\": 2},   \n",
    "]\n",
    "\n",
    "def getPeaks(smoothed_profile, max_noise=0.05):\n",
    "    len_smoothed_profile = len(smoothed_profile)\n",
    "\n",
    "    # Find peaks\n",
    "    peaks_idx = []\n",
    "    for prv_idx, val in enumerate(smoothed_profile[1:]):\n",
    "        if val > smoothed_profile[prv_idx] and (prv_idx + 3 > len_smoothed_profile or val >= smoothed_profile[prv_idx + 2]):\n",
    "            peaks_idx.append(prv_idx + 1)\n",
    "\n",
    "    # Retrieve weight of peaks \n",
    "    peaks_wheight = {idx: smoothed_profile[idx] for idx in peaks_idx}\n",
    "    for peak_idx in peaks_idx:\n",
    "        idx = peak_idx\n",
    "        is_desc = True\n",
    "        while is_desc:\n",
    "            if idx + 1 < len_smoothed_profile and smoothed_profile[idx] >= smoothed_profile[idx + 1] and smoothed_profile[idx] != 0:\n",
    "                peaks_wheight[peak_idx] += smoothed_profile[idx + 1]\n",
    "                idx += 1\n",
    "            else:\n",
    "                is_desc = False\n",
    "        idx = peak_idx\n",
    "        is_desc = True\n",
    "        while is_desc:\n",
    "            if idx > 0 and smoothed_profile[idx] >= smoothed_profile[idx - 1]and smoothed_profile[idx] != 0:\n",
    "                peaks_wheight[peak_idx] += smoothed_profile[idx - 1]\n",
    "                idx -= 1\n",
    "            else:\n",
    "                is_desc = False\n",
    "\n",
    "    # Filter peaks on weight\n",
    "    filtered_peaks = []\n",
    "    for pos, count in peaks_wheight.items():\n",
    "        if count >= max_noise*100:\n",
    "            filtered_peaks.append(pos)\n",
    "    \n",
    "    # Return peaks\n",
    "    return [idx for idx in filtered_peaks]\n",
    "\n",
    "\n",
    "rows = []\n",
    "titles = [\"sample\", \"locus\", \"method\", \"expected\", \"predicted\", \"prediction_status\"]\n",
    "for curr_model in models_param:\n",
    "    for spl in msi_eval:\n",
    "        diamic_name = spl.name.replace(\"_L001\", \"\").split(\"_\")[0]\n",
    "        if diamic_name in status_by_spl:\n",
    "            if spl.loci[curr_model[\"id\"]].results[method_name].getNbFrag() * 2 >= min_nb_reads:\n",
    "                series = spl.loci[curr_model[\"id\"]].results[method_name].getDensePrct(50, 200)\n",
    "                smoothed_series = gaussian_filter1d(series, smooth_sigma)\n",
    "                peaks = getPeaks(smoothed_series, 0.05)\n",
    "                expected_status = status_by_spl[diamic_name][curr_model[\"name\"]]\n",
    "                predicted_status = \"MSI\" if len(peaks) > 1 else \"MSS\"\n",
    "                rows.append([\n",
    "                    spl.name.replace(\"_L001\", \"\"),\n",
    "                    curr_model[\"name\"],\n",
    "                    \"smoothedPeaks\",\n",
    "                    expected_status,\n",
    "                    predicted_status,\n",
    "                    (expected_status == predicted_status)\n",
    "                ])\n",
    "                if expected_status != predicted_status:\n",
    "                    print(\n",
    "                        spl.name,\n",
    "                        curr_model[\"name\"],\n",
    "                        getPeaks(smoothed_series),\n",
    "                        find_peaks_cwt(series, np.arange(1,10), noise_perc=10),\n",
    "                        status_by_spl[diamic_name][curr_model[\"name\"]]\n",
    "                    )\n",
    "                    plt.plot(series, color='black')\n",
    "                    plt.plot(smoothed_series, color='green')\n",
    "                    plt.xlim(60, 110)\n",
    "                    plt.show()\n",
    "noML_df = pd.DataFrame.from_records(rows, columns=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation\n",
    "plt_by_locus = sns.factorplot(x=\"prediction_status\", hue=\"method\", col=\"locus\", data=noML_df, order=[True, False, \"NA\"], kind=\"count\")\n",
    "for ax in plt_by_locus.axes.flat:\n",
    "    locus_name = ax.get_title().split(\" \")[-1]\n",
    "    total_count = len(noML_df[(noML_df.locus == locus_name) & (noML_df.method == \"smoothedPeaks\")])\n",
    "    for item in ax.patches:\n",
    "        height = 0 if np.isnan(item.get_height()) else item.get_height()\n",
    "        ax.annotate(\n",
    "            '{:.1%}'.format(item.get_height()/total_count),\n",
    "            (\n",
    "                item.get_x() + item.get_width() / 2.,\n",
    "                item.get_height()\n",
    "            ),\n",
    "            ha='center', va='center', fontsize=11, color='gray', rotation=90, xytext=(0, 20),\n",
    "            textcoords='offset points')\n",
    "plt.subplots_adjust(top=0.8)\n",
    "plt.gcf().suptitle(\"Locus classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
